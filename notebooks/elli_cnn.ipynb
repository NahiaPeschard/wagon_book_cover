{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First CNN model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(X: pd.DataFrame, resize_size: tuple)-> np.ndarray:\n",
    "    \"\"\"\n",
    "    INPUT\n",
    "    X: the DataFrame from which we want to retrieve the image names in the 'Image_name' column\n",
    "\n",
    "    resize_size: a tuple to which dimensions the images should be resized\n",
    "    For example (100, 100) resizes the images to 100 x 100 pixels\n",
    "\n",
    "    OUTPUT\n",
    "    A numpy array of shape (number of images, pixel length, pixel height, color channels)\n",
    "    \"\"\"\n",
    "\n",
    "    output = []\n",
    "\n",
    "    for image_name in X['Image_name']:\n",
    "        image_path = os.path.join(\"../raw_data/Images/\", image_name)\n",
    "        image = Image.open(image_path)\n",
    "        image = image.resize(resize_size) # Resizing for speed purposes\n",
    "        image_array = np.array(image)\n",
    "        output.append(image_array)\n",
    "\n",
    "    return np.array(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(cache_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Retrieve data from `cache_path`\"\"\"\n",
    "    df = pd.read_csv(cache_path, delimiter=\" \", index_col=\"Id\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = get_data(\"../raw_data/train_x.csv\")\n",
    "y_train = get_data(\"../raw_data/train_y.csv\")\n",
    "\n",
    "X_test = get_data(\"../raw_data/test_x.csv\")\n",
    "y_test = get_data(\"../raw_data/test_y.csv\")\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "images_folder = os.path.join(parent_directory, 'raw_data', 'Images')\n",
    "images_filenames = os.listdir(images_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((51300, 2), (51300, 2)) ((5700, 2), (5700, 2))\n"
     ]
    }
   ],
   "source": [
    "print((X_train.shape, y_train.shape), (X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_main = preprocess_images(X_train, (224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = preprocess_images(X_test, (224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train['Genre_id']\n",
    "y_test = y_test['Genre_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Considering only 1/10th of the 50K+ images\n",
    "reduction_factor = 10\n",
    "\n",
    "# Choosing the random indices of small train set and small test set\n",
    "idx_train =  np.random.choice(len(X_train_main), round(len(X_train_main)/reduction_factor), replace=False)\n",
    "idx_test =  np.random.choice(len(X_test), round(len(X_test)/reduction_factor), replace=False)\n",
    "\n",
    "# Collecting the two subsamples images_train_small and images_test_small from images_train and images_test\n",
    "X_train_small = X_train_main[idx_train]\n",
    "X_test_small = X_test[idx_test]\n",
    "\n",
    "# and their corresponding labels\n",
    "y_train_small = y_train[idx_train]\n",
    "y_test_small = y_test[idx_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((51300, 224, 224, 3), (51300,)), ((5700, 224, 224, 3), (5700,)))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train_main.shape, y_train.shape), (X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((5130, 224, 224, 3), (5130,)), ((570, 224, 224, 3), (570,)))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train_small.shape, y_train_small.shape), (X_test_small.shape, y_test_small.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalise train and test data - **add this to preprocessing**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51300, 224, 224, 3)\n",
      "[[[[0.7882353  0.7647059  0.67058825]\n",
      "   [0.7882353  0.7647059  0.67058825]\n",
      "   [0.7921569  0.76862746 0.6745098 ]\n",
      "   ...\n",
      "   [0.76862746 0.74509805 0.65882355]\n",
      "   [0.76862746 0.74509805 0.65882355]\n",
      "   [0.76862746 0.74509805 0.65882355]]\n",
      "\n",
      "  [[0.7882353  0.7647059  0.67058825]\n",
      "   [0.7882353  0.7647059  0.67058825]\n",
      "   [0.78039217 0.75686276 0.6627451 ]\n",
      "   ...\n",
      "   [0.76862746 0.74509805 0.65882355]\n",
      "   [0.76862746 0.74509805 0.65882355]\n",
      "   [0.76862746 0.74509805 0.65882355]]\n",
      "\n",
      "  [[0.8117647  0.7882353  0.69411767]\n",
      "   [0.8039216  0.78039217 0.6862745 ]\n",
      "   [0.79607844 0.77254903 0.6784314 ]\n",
      "   ...\n",
      "   [0.76862746 0.74509805 0.65882355]\n",
      "   [0.76862746 0.74509805 0.65882355]\n",
      "   [0.76862746 0.74509805 0.65882355]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.36078432 0.38039216 0.2901961 ]\n",
      "   [0.3529412  0.37254903 0.28235295]\n",
      "   [0.34509805 0.3647059  0.27450982]\n",
      "   ...\n",
      "   [0.36862746 0.3882353  0.29803923]\n",
      "   [0.36862746 0.3882353  0.29803923]\n",
      "   [0.36862746 0.3882353  0.29803923]]\n",
      "\n",
      "  [[0.3529412  0.37254903 0.28235295]\n",
      "   [0.34901962 0.36862746 0.2784314 ]\n",
      "   [0.34901962 0.36862746 0.2784314 ]\n",
      "   ...\n",
      "   [0.36862746 0.3882353  0.29803923]\n",
      "   [0.36862746 0.3882353  0.29803923]\n",
      "   [0.36862746 0.3882353  0.29803923]]\n",
      "\n",
      "  [[0.35686275 0.3764706  0.28627452]\n",
      "   [0.3529412  0.37254903 0.28235295]\n",
      "   [0.35686275 0.3764706  0.28627452]\n",
      "   ...\n",
      "   [0.36862746 0.3882353  0.29803923]\n",
      "   [0.36862746 0.3882353  0.29803923]\n",
      "   [0.36862746 0.3882353  0.29803923]]]\n",
      "\n",
      "\n",
      " [[[0.5411765  0.827451   0.9647059 ]\n",
      "   [0.4627451  0.7490196  0.8862745 ]\n",
      "   [0.49411765 0.78039217 0.91764706]\n",
      "   ...\n",
      "   [0.5137255  0.76862746 0.89411765]\n",
      "   [0.50980395 0.7411765  0.85882354]\n",
      "   [0.7372549  0.95686275 1.        ]]\n",
      "\n",
      "  [[0.47058824 0.75686276 0.89411765]\n",
      "   [0.39607844 0.68235296 0.81960785]\n",
      "   [0.42352942 0.70980394 0.84705883]\n",
      "   ...\n",
      "   [0.44313726 0.69803923 0.8235294 ]\n",
      "   [0.4392157  0.67058825 0.7882353 ]\n",
      "   [0.6666667  0.8862745  0.99215686]]\n",
      "\n",
      "  [[0.49411765 0.78039217 0.91764706]\n",
      "   [0.41960785 0.7058824  0.84313726]\n",
      "   [0.44705883 0.73333335 0.87058824]\n",
      "   ...\n",
      "   [0.4627451  0.7176471  0.84313726]\n",
      "   [0.4627451  0.69411767 0.8117647 ]\n",
      "   [0.69411767 0.9137255  1.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.49803922 0.7490196  0.8862745 ]\n",
      "   [0.44705883 0.7019608  0.827451  ]\n",
      "   [0.44705883 0.69803923 0.8352941 ]\n",
      "   ...\n",
      "   [0.5058824  0.7254902  0.83137256]\n",
      "   [0.5176471  0.69411767 0.8156863 ]\n",
      "   [0.72156864 0.88235295 1.        ]]\n",
      "\n",
      "  [[0.49411765 0.7254902  0.84313726]\n",
      "   [0.44313726 0.6784314  0.7882353 ]\n",
      "   [0.4392157  0.67058825 0.7882353 ]\n",
      "   ...\n",
      "   [0.49411765 0.69803923 0.78039217]\n",
      "   [0.48235294 0.6666667  0.74509805]\n",
      "   [0.6666667  0.84313726 0.91764706]]\n",
      "\n",
      "  [[0.7372549  0.95686275 1.        ]\n",
      "   [0.6862745  0.90588236 1.        ]\n",
      "   [0.68235296 0.9019608  1.        ]\n",
      "   ...\n",
      "   [0.7176471  0.9254902  0.98039216]\n",
      "   [0.69411767 0.8784314  0.9411765 ]\n",
      "   [0.8666667  1.         1.        ]]]\n",
      "\n",
      "\n",
      " [[[0.6862745  0.5764706  0.49411765]\n",
      "   [0.4627451  0.27058825 0.21176471]\n",
      "   [0.6        0.22745098 0.21960784]\n",
      "   ...\n",
      "   [0.6156863  0.21176471 0.19215687]\n",
      "   [0.6156863  0.21176471 0.19215687]\n",
      "   [0.6666667  0.2627451  0.24313726]]\n",
      "\n",
      "  [[0.5686275  0.4392157  0.3647059 ]\n",
      "   [0.34117648 0.12156863 0.07058824]\n",
      "   [0.45490196 0.0627451  0.05490196]\n",
      "   ...\n",
      "   [0.4627451  0.03921569 0.02745098]\n",
      "   [0.46666667 0.04313726 0.03137255]\n",
      "   [0.5176471  0.09411765 0.08235294]]\n",
      "\n",
      "  [[0.627451   0.45882353 0.39215687]\n",
      "   [0.37254903 0.12156863 0.07450981]\n",
      "   [0.47058824 0.04705882 0.04705882]\n",
      "   ...\n",
      "   [0.47843137 0.02352941 0.02352941]\n",
      "   [0.48235294 0.02745098 0.02745098]\n",
      "   [0.53333336 0.07843138 0.07843138]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.627451   0.48235294 0.4509804 ]\n",
      "   [0.2901961  0.05882353 0.04313726]\n",
      "   [0.42352942 0.01176471 0.03137255]\n",
      "   ...\n",
      "   [0.5058824  0.         0.        ]\n",
      "   [0.50980395 0.         0.        ]\n",
      "   [0.5647059  0.05490196 0.05490196]]\n",
      "\n",
      "  [[0.6313726  0.4862745  0.45490196]\n",
      "   [0.29411766 0.0627451  0.04705882]\n",
      "   [0.43137255 0.01960784 0.03921569]\n",
      "   ...\n",
      "   [0.52156866 0.01176471 0.01176471]\n",
      "   [0.5254902  0.01568628 0.01568628]\n",
      "   [0.5803922  0.07058824 0.07058824]]\n",
      "\n",
      "  [[0.67058825 0.5254902  0.49411765]\n",
      "   [0.34117648 0.10980392 0.09411765]\n",
      "   [0.48235294 0.07058824 0.09019608]\n",
      "   ...\n",
      "   [0.5764706  0.06666667 0.06666667]\n",
      "   [0.58431375 0.07450981 0.07450981]\n",
      "   [0.63529414 0.1254902  0.1254902 ]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.61960787 0.6313726  0.6509804 ]\n",
      "   [0.         0.         0.01960784]\n",
      "   [0.0627451  0.07450981 0.09411765]\n",
      "   ...\n",
      "   [0.07843138 0.09411765 0.13725491]\n",
      "   [0.07843138 0.09411765 0.13725491]\n",
      "   [0.07843138 0.09411765 0.13725491]]\n",
      "\n",
      "  [[0.61960787 0.6313726  0.6509804 ]\n",
      "   [0.         0.         0.01960784]\n",
      "   [0.0627451  0.07450981 0.09411765]\n",
      "   ...\n",
      "   [0.07843138 0.09411765 0.13725491]\n",
      "   [0.07843138 0.09411765 0.13725491]\n",
      "   [0.07843138 0.09411765 0.13725491]]\n",
      "\n",
      "  [[0.61960787 0.6313726  0.6509804 ]\n",
      "   [0.         0.         0.01960784]\n",
      "   [0.06666667 0.07843138 0.09803922]\n",
      "   ...\n",
      "   [0.08235294 0.09803922 0.14117648]\n",
      "   [0.08235294 0.09803922 0.14117648]\n",
      "   [0.08235294 0.09803922 0.14117648]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.63529414 0.6392157  0.654902  ]\n",
      "   [0.         0.         0.01568628]\n",
      "   [0.07058824 0.07450981 0.09019608]\n",
      "   ...\n",
      "   [0.07450981 0.07058824 0.09019608]\n",
      "   [0.07450981 0.07058824 0.09019608]\n",
      "   [0.07450981 0.07058824 0.09019608]]\n",
      "\n",
      "  [[0.63529414 0.6392157  0.654902  ]\n",
      "   [0.         0.         0.01568628]\n",
      "   [0.07058824 0.07450981 0.09019608]\n",
      "   ...\n",
      "   [0.07450981 0.07058824 0.09019608]\n",
      "   [0.07450981 0.07058824 0.09019608]\n",
      "   [0.07450981 0.07058824 0.09019608]]\n",
      "\n",
      "  [[0.63529414 0.6392157  0.654902  ]\n",
      "   [0.         0.         0.01568628]\n",
      "   [0.07058824 0.07450981 0.09019608]\n",
      "   ...\n",
      "   [0.07450981 0.07058824 0.09019608]\n",
      "   [0.07450981 0.07058824 0.09019608]\n",
      "   [0.07450981 0.07058824 0.09019608]]]\n",
      "\n",
      "\n",
      " [[[0.24313726 0.23921569 0.23137255]\n",
      "   [0.24313726 0.23921569 0.23137255]\n",
      "   [0.24313726 0.23921569 0.23137255]\n",
      "   ...\n",
      "   [0.24313726 0.23921569 0.23137255]\n",
      "   [0.24313726 0.23921569 0.23137255]\n",
      "   [0.24313726 0.23921569 0.23137255]]\n",
      "\n",
      "  [[0.24313726 0.23921569 0.23137255]\n",
      "   [0.24313726 0.23921569 0.23137255]\n",
      "   [0.24313726 0.23921569 0.23137255]\n",
      "   ...\n",
      "   [0.24313726 0.23921569 0.23137255]\n",
      "   [0.24313726 0.23921569 0.23137255]\n",
      "   [0.24313726 0.23921569 0.23137255]]\n",
      "\n",
      "  [[0.24313726 0.23921569 0.23137255]\n",
      "   [0.24313726 0.23921569 0.23137255]\n",
      "   [0.24313726 0.23921569 0.23137255]\n",
      "   ...\n",
      "   [0.24313726 0.23921569 0.23137255]\n",
      "   [0.24313726 0.23921569 0.23137255]\n",
      "   [0.24313726 0.23921569 0.23137255]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.21960784 0.20392157 0.2       ]\n",
      "   [0.21960784 0.20392157 0.2       ]\n",
      "   [0.21960784 0.20392157 0.2       ]\n",
      "   ...\n",
      "   [0.21960784 0.20392157 0.2       ]\n",
      "   [0.21960784 0.20392157 0.2       ]\n",
      "   [0.21960784 0.20392157 0.2       ]]\n",
      "\n",
      "  [[0.21960784 0.20392157 0.2       ]\n",
      "   [0.21960784 0.20392157 0.2       ]\n",
      "   [0.21960784 0.20392157 0.2       ]\n",
      "   ...\n",
      "   [0.21960784 0.20392157 0.2       ]\n",
      "   [0.21960784 0.20392157 0.2       ]\n",
      "   [0.21960784 0.20392157 0.2       ]]\n",
      "\n",
      "  [[0.21960784 0.20392157 0.2       ]\n",
      "   [0.21960784 0.20392157 0.2       ]\n",
      "   [0.21960784 0.20392157 0.2       ]\n",
      "   ...\n",
      "   [0.21960784 0.20392157 0.2       ]\n",
      "   [0.21960784 0.20392157 0.2       ]\n",
      "   [0.21960784 0.20392157 0.2       ]]]\n",
      "\n",
      "\n",
      " [[[0.8156863  0.19607843 0.09019608]\n",
      "   [0.9254902  0.30588236 0.2       ]\n",
      "   [0.95686275 0.3372549  0.23137255]\n",
      "   ...\n",
      "   [0.8666667  0.24705882 0.14117648]\n",
      "   [0.9019608  0.28235295 0.1764706 ]\n",
      "   [0.9098039  0.2901961  0.18431373]]\n",
      "\n",
      "  [[0.83137256 0.21176471 0.10588235]\n",
      "   [0.8980392  0.2784314  0.17254902]\n",
      "   [0.9490196  0.32941177 0.22352941]\n",
      "   ...\n",
      "   [0.8627451  0.24313726 0.13725491]\n",
      "   [0.85490197 0.23529412 0.12941177]\n",
      "   [0.8509804  0.23137255 0.1254902 ]]\n",
      "\n",
      "  [[0.8627451  0.24313726 0.13725491]\n",
      "   [0.85490197 0.23529412 0.12941177]\n",
      "   [0.92941177 0.30980393 0.20392157]\n",
      "   ...\n",
      "   [0.9647059  0.34509805 0.23921569]\n",
      "   [0.93333334 0.3137255  0.20784314]\n",
      "   [0.88235295 0.2627451  0.15686275]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.84313726 0.23921569 0.13725491]\n",
      "   [0.8784314  0.27450982 0.17254902]\n",
      "   [0.90588236 0.3019608  0.2       ]\n",
      "   ...\n",
      "   [0.8901961  0.28627452 0.18431373]\n",
      "   [0.8156863  0.22352941 0.11764706]\n",
      "   [0.8392157  0.24705882 0.14117648]]\n",
      "\n",
      "  [[0.88235295 0.2784314  0.1764706 ]\n",
      "   [0.9529412  0.34901962 0.24705882]\n",
      "   [0.93333334 0.32941177 0.22745098]\n",
      "   ...\n",
      "   [0.88235295 0.2784314  0.1764706 ]\n",
      "   [0.8156863  0.22352941 0.11764706]\n",
      "   [0.84313726 0.2509804  0.14509805]]\n",
      "\n",
      "  [[0.92941177 0.3254902  0.22352941]\n",
      "   [0.85490197 0.2509804  0.14901961]\n",
      "   [0.8        0.19607843 0.09411765]\n",
      "   ...\n",
      "   [0.88235295 0.2784314  0.1764706 ]\n",
      "   [0.8156863  0.22352941 0.11764706]\n",
      "   [0.84313726 0.2509804  0.14509805]]]]\n",
      "(5700, 224, 224, 3)\n",
      "[[[[0.49019608 0.65882355 0.8352941 ]\n",
      "   [0.49019608 0.65882355 0.8352941 ]\n",
      "   [0.49019608 0.65882355 0.8352941 ]\n",
      "   ...\n",
      "   [0.49019608 0.65882355 0.8352941 ]\n",
      "   [0.49019608 0.65882355 0.8352941 ]\n",
      "   [0.49019608 0.65882355 0.8352941 ]]\n",
      "\n",
      "  [[0.49019608 0.65882355 0.8352941 ]\n",
      "   [0.49019608 0.65882355 0.8352941 ]\n",
      "   [0.49019608 0.65882355 0.8352941 ]\n",
      "   ...\n",
      "   [0.49019608 0.65882355 0.8352941 ]\n",
      "   [0.49019608 0.65882355 0.8352941 ]\n",
      "   [0.49019608 0.65882355 0.8352941 ]]\n",
      "\n",
      "  [[0.49019608 0.65882355 0.8352941 ]\n",
      "   [0.49019608 0.65882355 0.8352941 ]\n",
      "   [0.49019608 0.65882355 0.8352941 ]\n",
      "   ...\n",
      "   [0.49019608 0.65882355 0.8352941 ]\n",
      "   [0.49019608 0.65882355 0.8352941 ]\n",
      "   [0.49019608 0.65882355 0.8352941 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.40784314 0.20784314 0.5019608 ]\n",
      "   [0.40784314 0.20784314 0.5019608 ]\n",
      "   [0.40784314 0.20784314 0.5019608 ]\n",
      "   ...\n",
      "   [0.40784314 0.20784314 0.5019608 ]\n",
      "   [0.40784314 0.20784314 0.5019608 ]\n",
      "   [0.40784314 0.20784314 0.5019608 ]]\n",
      "\n",
      "  [[0.40784314 0.20784314 0.5019608 ]\n",
      "   [0.40784314 0.20784314 0.5019608 ]\n",
      "   [0.40784314 0.20784314 0.5019608 ]\n",
      "   ...\n",
      "   [0.40784314 0.20784314 0.5019608 ]\n",
      "   [0.40784314 0.20784314 0.5019608 ]\n",
      "   [0.40784314 0.20784314 0.5019608 ]]\n",
      "\n",
      "  [[0.40784314 0.20784314 0.5019608 ]\n",
      "   [0.40784314 0.20784314 0.5019608 ]\n",
      "   [0.40784314 0.20784314 0.5019608 ]\n",
      "   ...\n",
      "   [0.40784314 0.20784314 0.5019608 ]\n",
      "   [0.40784314 0.20784314 0.5019608 ]\n",
      "   [0.40784314 0.20784314 0.5019608 ]]]\n",
      "\n",
      "\n",
      " [[[0.01960784 0.34901962 0.58431375]\n",
      "   [0.01960784 0.34901962 0.58431375]\n",
      "   [0.01568628 0.3529412  0.5764706 ]\n",
      "   ...\n",
      "   [0.00392157 0.3647059  0.5647059 ]\n",
      "   [0.02745098 0.3529412  0.54901963]\n",
      "   [0.03137255 0.3529412  0.5411765 ]]\n",
      "\n",
      "  [[0.01960784 0.34901962 0.58431375]\n",
      "   [0.01568628 0.3529412  0.58431375]\n",
      "   [0.01568628 0.3529412  0.5764706 ]\n",
      "   ...\n",
      "   [0.00392157 0.3647059  0.5647059 ]\n",
      "   [0.01960784 0.35686275 0.5568628 ]\n",
      "   [0.02745098 0.3529412  0.54901963]]\n",
      "\n",
      "  [[0.01568628 0.3529412  0.5764706 ]\n",
      "   [0.00784314 0.35686275 0.5764706 ]\n",
      "   [0.00784314 0.35686275 0.5764706 ]\n",
      "   ...\n",
      "   [0.00392157 0.36078432 0.5686275 ]\n",
      "   [0.01568628 0.35686275 0.5647059 ]\n",
      "   [0.01960784 0.35686275 0.5568628 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[1.         0.85490197 0.22352941]\n",
      "   [0.98039216 0.77254903 0.16078432]\n",
      "   [0.9098039  0.6666667  0.09803922]\n",
      "   ...\n",
      "   [0.9019608  0.5137255  0.11764706]\n",
      "   [0.9372549  0.5568628  0.15294118]\n",
      "   [0.96862745 0.59607846 0.18039216]]\n",
      "\n",
      "  [[1.         0.81960785 0.23529412]\n",
      "   [1.         0.7882353  0.22352941]\n",
      "   [0.9882353  0.73333335 0.22352941]\n",
      "   ...\n",
      "   [0.96862745 0.52156866 0.18431373]\n",
      "   [0.9607843  0.5058824  0.16470589]\n",
      "   [0.9882353  0.5254902  0.18039216]]\n",
      "\n",
      "  [[0.9411765  0.7529412  0.1882353 ]\n",
      "   [0.99607843 0.7882353  0.24705882]\n",
      "   [1.         0.8117647  0.33333334]\n",
      "   ...\n",
      "   [0.96862745 0.5019608  0.1882353 ]\n",
      "   [0.98039216 0.48235294 0.18039216]\n",
      "   [1.         0.5058824  0.2       ]]]\n",
      "\n",
      "\n",
      " [[[0.49019608 0.49411765 0.50980395]\n",
      "   [0.49019608 0.49411765 0.50980395]\n",
      "   [0.49019608 0.49411765 0.50980395]\n",
      "   ...\n",
      "   [0.5137255  0.5176471  0.5254902 ]\n",
      "   [0.5176471  0.5176471  0.5176471 ]\n",
      "   [0.5137255  0.5137255  0.5137255 ]]\n",
      "\n",
      "  [[0.49019608 0.49411765 0.50980395]\n",
      "   [0.49019608 0.49411765 0.50980395]\n",
      "   [0.49019608 0.49411765 0.50980395]\n",
      "   ...\n",
      "   [0.5137255  0.5176471  0.5254902 ]\n",
      "   [0.5176471  0.5176471  0.5176471 ]\n",
      "   [0.5137255  0.5137255  0.5137255 ]]\n",
      "\n",
      "  [[0.49019608 0.49411765 0.50980395]\n",
      "   [0.49019608 0.49411765 0.50980395]\n",
      "   [0.49019608 0.49411765 0.50980395]\n",
      "   ...\n",
      "   [0.5137255  0.5176471  0.5254902 ]\n",
      "   [0.50980395 0.5176471  0.5137255 ]\n",
      "   [0.50980395 0.5176471  0.5137255 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.43529412 0.4392157  0.45490196]\n",
      "   [0.43529412 0.4392157  0.45490196]\n",
      "   [0.4392157  0.44313726 0.45882353]\n",
      "   ...\n",
      "   [0.17254902 0.16470589 0.16862746]\n",
      "   [0.17254902 0.16470589 0.16862746]\n",
      "   [0.16862746 0.16078432 0.16470589]]\n",
      "\n",
      "  [[0.44313726 0.44705883 0.4627451 ]\n",
      "   [0.44313726 0.44705883 0.4627451 ]\n",
      "   [0.44313726 0.44705883 0.4627451 ]\n",
      "   ...\n",
      "   [0.18431373 0.1764706  0.18039216]\n",
      "   [0.18039216 0.17254902 0.1764706 ]\n",
      "   [0.1764706  0.16862746 0.17254902]]\n",
      "\n",
      "  [[0.45490196 0.45882353 0.4745098 ]\n",
      "   [0.4509804  0.45490196 0.47058824]\n",
      "   [0.44705883 0.4509804  0.46666667]\n",
      "   ...\n",
      "   [0.1882353  0.18039216 0.18431373]\n",
      "   [0.18431373 0.1764706  0.18039216]\n",
      "   [0.18039216 0.17254902 0.1764706 ]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.9254902  0.8392157  0.40392157]\n",
      "   [0.9254902  0.8392157  0.40392157]\n",
      "   [0.9254902  0.8392157  0.40392157]\n",
      "   ...\n",
      "   [0.89411765 0.79607844 0.34117648]\n",
      "   [0.90588236 0.80784315 0.3529412 ]\n",
      "   [0.9137255  0.8156863  0.36078432]]\n",
      "\n",
      "  [[0.92156863 0.8352941  0.4       ]\n",
      "   [0.91764706 0.83137256 0.39607844]\n",
      "   [0.91764706 0.83137256 0.39607844]\n",
      "   ...\n",
      "   [0.8862745  0.7882353  0.33333334]\n",
      "   [0.89411765 0.79607844 0.34117648]\n",
      "   [0.9019608  0.8039216  0.34901962]]\n",
      "\n",
      "  [[0.9137255  0.827451   0.39215687]\n",
      "   [0.9098039  0.8235294  0.3882353 ]\n",
      "   [0.90588236 0.81960785 0.39215687]\n",
      "   ...\n",
      "   [0.8745098  0.77254903 0.32941177]\n",
      "   [0.88235295 0.78431374 0.32941177]\n",
      "   [0.8901961  0.7921569  0.3372549 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.8117647  0.78039217 0.5372549 ]\n",
      "   [0.8235294  0.7764706  0.5411765 ]\n",
      "   [0.84705883 0.7647059  0.54901963]\n",
      "   ...\n",
      "   [0.14117648 0.15686275 0.05882353]\n",
      "   [0.19215687 0.20784314 0.10196079]\n",
      "   [0.25490198 0.27450982 0.15686275]]\n",
      "\n",
      "  [[0.75686276 0.7529412  0.49803922]\n",
      "   [0.8235294  0.8        0.5568628 ]\n",
      "   [0.84705883 0.7882353  0.5647059 ]\n",
      "   ...\n",
      "   [0.16470589 0.19215687 0.07058824]\n",
      "   [0.26666668 0.29411766 0.16470589]\n",
      "   [0.3529412  0.38039216 0.24705882]]\n",
      "\n",
      "  [[0.78039217 0.78431374 0.5254902 ]\n",
      "   [0.84313726 0.827451   0.5764706 ]\n",
      "   [0.8235294  0.7764706  0.54901963]\n",
      "   ...\n",
      "   [0.34117648 0.36862746 0.23921569]\n",
      "   [0.33333334 0.37254903 0.23529412]\n",
      "   [0.25882354 0.29803923 0.15294118]]]\n",
      "\n",
      "\n",
      " [[[0.04313726 0.04313726 0.03529412]\n",
      "   [0.03137255 0.03137255 0.02352941]\n",
      "   [0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.01960784 0.01960784 0.01960784]\n",
      "   [0.01960784 0.01960784 0.01960784]\n",
      "   [0.01960784 0.01960784 0.01960784]]\n",
      "\n",
      "  [[0.03921569 0.03921569 0.03137255]\n",
      "   [0.02745098 0.02745098 0.01960784]\n",
      "   [0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.01960784 0.01960784 0.01960784]\n",
      "   [0.01960784 0.01960784 0.01960784]\n",
      "   [0.01960784 0.01960784 0.01960784]]\n",
      "\n",
      "  [[0.03137255 0.03137255 0.02352941]\n",
      "   [0.01960784 0.01960784 0.01176471]\n",
      "   [0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.01960784 0.01960784 0.01960784]\n",
      "   [0.01960784 0.01960784 0.01960784]\n",
      "   [0.01960784 0.01960784 0.01960784]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.8352941  0.78431374 0.7490196 ]\n",
      "   [0.83137256 0.78431374 0.7372549 ]\n",
      "   [0.8156863  0.76862746 0.72156864]\n",
      "   ...\n",
      "   [0.8117647  0.7647059  0.7176471 ]\n",
      "   [0.8        0.7529412  0.7058824 ]\n",
      "   [0.7921569  0.74509805 0.69803923]]\n",
      "\n",
      "  [[0.80784315 0.78039217 0.7411765 ]\n",
      "   [0.8039216  0.78039217 0.73333335]\n",
      "   [0.7882353  0.7647059  0.7176471 ]\n",
      "   ...\n",
      "   [0.81960785 0.77254903 0.7254902 ]\n",
      "   [0.81960785 0.77254903 0.7254902 ]\n",
      "   [0.8235294  0.7764706  0.7294118 ]]\n",
      "\n",
      "  [[0.80784315 0.8        0.7529412 ]\n",
      "   [0.8039216  0.79607844 0.7490196 ]\n",
      "   [0.79607844 0.78039217 0.73333335]\n",
      "   ...\n",
      "   [0.81960785 0.77254903 0.7254902 ]\n",
      "   [0.827451   0.78039217 0.73333335]\n",
      "   [0.8352941  0.7882353  0.7411765 ]]]\n",
      "\n",
      "\n",
      " [[[0.15294118 0.14509805 0.23921569]\n",
      "   [0.15294118 0.14509805 0.23921569]\n",
      "   [0.15686275 0.14901961 0.24313726]\n",
      "   ...\n",
      "   [0.12941177 0.12941177 0.16862746]\n",
      "   [0.12941177 0.12941177 0.16862746]\n",
      "   [0.12941177 0.12941177 0.16862746]]\n",
      "\n",
      "  [[0.15294118 0.14509805 0.23921569]\n",
      "   [0.15294118 0.14509805 0.23921569]\n",
      "   [0.15686275 0.14901961 0.24313726]\n",
      "   ...\n",
      "   [0.12941177 0.12941177 0.16862746]\n",
      "   [0.12941177 0.12941177 0.16862746]\n",
      "   [0.12941177 0.12941177 0.16862746]]\n",
      "\n",
      "  [[0.14901961 0.14117648 0.22745098]\n",
      "   [0.15294118 0.14509805 0.23137255]\n",
      "   [0.15294118 0.14509805 0.23137255]\n",
      "   ...\n",
      "   [0.12941177 0.12941177 0.16862746]\n",
      "   [0.12941177 0.12941177 0.16862746]\n",
      "   [0.1254902  0.1254902  0.16470589]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.5921569  0.63529414 0.8156863 ]\n",
      "   [0.54901963 0.5921569  0.77254903]\n",
      "   [0.49411765 0.5372549  0.7176471 ]\n",
      "   ...\n",
      "   [0.25490198 0.3372549  0.6       ]\n",
      "   [0.24705882 0.32941177 0.5921569 ]\n",
      "   [0.24705882 0.32941177 0.5921569 ]]\n",
      "\n",
      "  [[0.6627451  0.7058824  0.88235295]\n",
      "   [0.43137255 0.4745098  0.6509804 ]\n",
      "   [0.49411765 0.5372549  0.7137255 ]\n",
      "   ...\n",
      "   [0.25490198 0.3372549  0.6       ]\n",
      "   [0.2509804  0.33333334 0.59607846]\n",
      "   [0.25490198 0.3372549  0.6       ]]\n",
      "\n",
      "  [[0.4627451  0.5058824  0.68235296]\n",
      "   [0.3372549  0.38039216 0.5568628 ]\n",
      "   [0.43529412 0.47843137 0.654902  ]\n",
      "   ...\n",
      "   [0.2509804  0.33333334 0.59607846]\n",
      "   [0.2509804  0.33333334 0.59607846]\n",
      "   [0.25882354 0.34117648 0.6039216 ]]]]\n"
     ]
    }
   ],
   "source": [
    "# Define the batch size\n",
    "batch_size = 100\n",
    "\n",
    "# Create a list of arrays to process\n",
    "arrays = [X_train_main, X_test]\n",
    "lst = []\n",
    "# Iterate over the arrays\n",
    "for i, arr in enumerate(arrays):\n",
    "    # Get the total number of images\n",
    "    num_images = len(arr)\n",
    "\n",
    "    # Calculate the total number of batches\n",
    "    num_batches = int(np.ceil(num_images / batch_size))\n",
    "\n",
    "    # Create an empty array to store the normalized images\n",
    "    normalized_arr = np.empty_like(arr, dtype=np.float32)\n",
    "\n",
    "    # Process images in batches\n",
    "    for j in range(num_batches):\n",
    "        start_idx = j * batch_size\n",
    "        end_idx = min((j + 1) * batch_size, num_images)\n",
    "\n",
    "        # Normalize the batch of images\n",
    "        batch_images = arr[start_idx:end_idx] / 255.\n",
    "\n",
    "        # Store the normalized batch in the result array\n",
    "        normalized_arr[start_idx:end_idx] = batch_images\n",
    "\n",
    "    # Assign the normalized array back to the original variable\n",
    "    arr = normalized_arr\n",
    "    lst.append(arr)\n",
    "\n",
    "    # Print the shape of the array after normalization\n",
    "    print(arr.shape)\n",
    "    print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = lst[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = lst[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51300, 224, 224, 3) (5700, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same for Small Dataset\n",
    "\n",
    "(X_train_small.shape, y_train_small.shape), (X_test_small.shape, y_test_small.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5130, 224, 224, 3)\n",
      "[[[[0.2        0.18431373 0.17254902]\n",
      "   [0.2        0.18431373 0.17254902]\n",
      "   [0.2        0.18431373 0.17254902]\n",
      "   ...\n",
      "   [0.18039216 0.1764706  0.16862746]\n",
      "   [0.18039216 0.1764706  0.16862746]\n",
      "   [0.18039216 0.1764706  0.16862746]]\n",
      "\n",
      "  [[0.2        0.18431373 0.17254902]\n",
      "   [0.2        0.18431373 0.17254902]\n",
      "   [0.2        0.18431373 0.17254902]\n",
      "   ...\n",
      "   [0.18039216 0.1764706  0.16862746]\n",
      "   [0.18039216 0.1764706  0.16862746]\n",
      "   [0.18039216 0.1764706  0.16862746]]\n",
      "\n",
      "  [[0.2        0.18431373 0.17254902]\n",
      "   [0.2        0.18431373 0.17254902]\n",
      "   [0.2        0.18431373 0.17254902]\n",
      "   ...\n",
      "   [0.18039216 0.1764706  0.16862746]\n",
      "   [0.18039216 0.1764706  0.16862746]\n",
      "   [0.18039216 0.1764706  0.16862746]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.14509805 0.18039216 0.16078432]\n",
      "   [0.14509805 0.18039216 0.16078432]\n",
      "   [0.15294118 0.1764706  0.16078432]\n",
      "   ...\n",
      "   [0.16078432 0.16078432 0.16078432]\n",
      "   [0.16470589 0.16470589 0.16470589]\n",
      "   [0.16470589 0.16470589 0.16470589]]\n",
      "\n",
      "  [[0.15294118 0.18039216 0.15294118]\n",
      "   [0.15294118 0.18039216 0.15294118]\n",
      "   [0.15686275 0.1764706  0.15294118]\n",
      "   ...\n",
      "   [0.16078432 0.16078432 0.16078432]\n",
      "   [0.16470589 0.16470589 0.16470589]\n",
      "   [0.16470589 0.16470589 0.16470589]]\n",
      "\n",
      "  [[0.15686275 0.1764706  0.14901961]\n",
      "   [0.15686275 0.1764706  0.14901961]\n",
      "   [0.16470589 0.17254902 0.15294118]\n",
      "   ...\n",
      "   [0.16078432 0.16078432 0.16078432]\n",
      "   [0.16078432 0.16078432 0.16078432]\n",
      "   [0.16470589 0.16470589 0.16470589]]]\n",
      "\n",
      "\n",
      " [[[0.92156863 0.6901961  0.3372549 ]\n",
      "   [0.89411765 0.67058825 0.31764707]\n",
      "   [0.8901961  0.68235296 0.3137255 ]\n",
      "   ...\n",
      "   [0.85490197 0.54901963 0.4627451 ]\n",
      "   [0.85490197 0.54901963 0.4627451 ]\n",
      "   [1.         0.69411767 0.6039216 ]]\n",
      "\n",
      "  [[0.95686275 0.7254902  0.37254903]\n",
      "   [0.91764706 0.7019608  0.34509805]\n",
      "   [0.9019608  0.69411767 0.3254902 ]\n",
      "   ...\n",
      "   [0.77254903 0.49411765 0.41568628]\n",
      "   [0.88235295 0.6039216  0.5176471 ]\n",
      "   [0.8862745  0.61960787 0.5294118 ]]\n",
      "\n",
      "  [[0.8980392  0.6784314  0.3137255 ]\n",
      "   [0.8745098  0.6666667  0.29803923]\n",
      "   [0.87058824 0.6745098  0.3019608 ]\n",
      "   ...\n",
      "   [0.45882353 0.24313726 0.16470589]\n",
      "   [0.827451   0.62352943 0.5411765 ]\n",
      "   [0.7882353  0.5882353  0.5058824 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.16078432 0.11372549 0.12941177]\n",
      "   [0.14901961 0.10588235 0.12156863]\n",
      "   [0.14117648 0.10196079 0.10588235]\n",
      "   ...\n",
      "   [0.13725491 0.12156863 0.1254902 ]\n",
      "   [0.13725491 0.12156863 0.1254902 ]\n",
      "   [0.13725491 0.12156863 0.1254902 ]]\n",
      "\n",
      "  [[0.14901961 0.10980392 0.11372549]\n",
      "   [0.15294118 0.11372549 0.11764706]\n",
      "   [0.15686275 0.10980392 0.11764706]\n",
      "   ...\n",
      "   [0.13725491 0.12156863 0.1254902 ]\n",
      "   [0.13725491 0.12156863 0.1254902 ]\n",
      "   [0.13725491 0.12156863 0.1254902 ]]\n",
      "\n",
      "  [[0.13725491 0.09803922 0.10196079]\n",
      "   [0.15294118 0.11372549 0.11764706]\n",
      "   [0.17254902 0.11372549 0.1254902 ]\n",
      "   ...\n",
      "   [0.13725491 0.12156863 0.1254902 ]\n",
      "   [0.13725491 0.12156863 0.1254902 ]\n",
      "   [0.13725491 0.12156863 0.1254902 ]]]\n",
      "\n",
      "\n",
      " [[[0.16078432 0.16078432 0.16078432]\n",
      "   [0.16078432 0.16078432 0.16078432]\n",
      "   [0.16078432 0.16078432 0.16078432]\n",
      "   ...\n",
      "   [0.16078432 0.16078432 0.16078432]\n",
      "   [0.16078432 0.16078432 0.16078432]\n",
      "   [0.16078432 0.16078432 0.16078432]]\n",
      "\n",
      "  [[0.16078432 0.16078432 0.16078432]\n",
      "   [0.16078432 0.16078432 0.16078432]\n",
      "   [0.16078432 0.16078432 0.16078432]\n",
      "   ...\n",
      "   [0.16078432 0.16078432 0.16078432]\n",
      "   [0.16078432 0.16078432 0.16078432]\n",
      "   [0.16078432 0.16078432 0.16078432]]\n",
      "\n",
      "  [[0.16078432 0.16078432 0.16078432]\n",
      "   [0.16078432 0.16078432 0.16078432]\n",
      "   [0.16078432 0.16078432 0.16078432]\n",
      "   ...\n",
      "   [0.16078432 0.16078432 0.16078432]\n",
      "   [0.16078432 0.16078432 0.16078432]\n",
      "   [0.16078432 0.16078432 0.16078432]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.16078432 0.16078432 0.16078432]\n",
      "   [0.16078432 0.16078432 0.16078432]\n",
      "   [0.16078432 0.16078432 0.16078432]\n",
      "   ...\n",
      "   [0.16078432 0.16078432 0.16078432]\n",
      "   [0.16078432 0.16078432 0.16078432]\n",
      "   [0.16078432 0.16078432 0.16078432]]\n",
      "\n",
      "  [[0.16078432 0.16078432 0.16078432]\n",
      "   [0.16078432 0.16078432 0.16078432]\n",
      "   [0.16078432 0.16078432 0.16078432]\n",
      "   ...\n",
      "   [0.16078432 0.16078432 0.16078432]\n",
      "   [0.16078432 0.16078432 0.16078432]\n",
      "   [0.16078432 0.16078432 0.16078432]]\n",
      "\n",
      "  [[0.16078432 0.16078432 0.16078432]\n",
      "   [0.16078432 0.16078432 0.16078432]\n",
      "   [0.16078432 0.16078432 0.16078432]\n",
      "   ...\n",
      "   [0.16078432 0.16078432 0.16078432]\n",
      "   [0.16078432 0.16078432 0.16078432]\n",
      "   [0.16078432 0.16078432 0.16078432]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.72156864 0.8745098  0.7607843 ]\n",
      "   [0.72156864 0.8745098  0.7607843 ]\n",
      "   [0.72156864 0.8745098  0.7607843 ]\n",
      "   ...\n",
      "   [0.72156864 0.8745098  0.7607843 ]\n",
      "   [0.72156864 0.8745098  0.7607843 ]\n",
      "   [0.72156864 0.8745098  0.7607843 ]]\n",
      "\n",
      "  [[0.72156864 0.8745098  0.7607843 ]\n",
      "   [0.72156864 0.8745098  0.7607843 ]\n",
      "   [0.72156864 0.8745098  0.7607843 ]\n",
      "   ...\n",
      "   [0.72156864 0.8745098  0.7607843 ]\n",
      "   [0.72156864 0.8745098  0.7607843 ]\n",
      "   [0.72156864 0.8745098  0.7607843 ]]\n",
      "\n",
      "  [[0.72156864 0.8745098  0.7607843 ]\n",
      "   [0.72156864 0.8745098  0.7607843 ]\n",
      "   [0.72156864 0.8745098  0.7607843 ]\n",
      "   ...\n",
      "   [0.72156864 0.8745098  0.7607843 ]\n",
      "   [0.72156864 0.8745098  0.7607843 ]\n",
      "   [0.72156864 0.8745098  0.7607843 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.72156864 0.8745098  0.7607843 ]\n",
      "   [0.72156864 0.8745098  0.7607843 ]\n",
      "   [0.72156864 0.8745098  0.7607843 ]\n",
      "   ...\n",
      "   [0.72156864 0.8745098  0.7607843 ]\n",
      "   [0.72156864 0.8745098  0.7607843 ]\n",
      "   [0.72156864 0.8745098  0.7607843 ]]\n",
      "\n",
      "  [[0.72156864 0.8745098  0.7607843 ]\n",
      "   [0.72156864 0.8745098  0.7607843 ]\n",
      "   [0.72156864 0.8745098  0.7607843 ]\n",
      "   ...\n",
      "   [0.72156864 0.8745098  0.7607843 ]\n",
      "   [0.72156864 0.8745098  0.7607843 ]\n",
      "   [0.72156864 0.8745098  0.7607843 ]]\n",
      "\n",
      "  [[0.72156864 0.8745098  0.7607843 ]\n",
      "   [0.72156864 0.8745098  0.7607843 ]\n",
      "   [0.72156864 0.8745098  0.7607843 ]\n",
      "   ...\n",
      "   [0.72156864 0.8745098  0.7607843 ]\n",
      "   [0.72156864 0.8745098  0.7607843 ]\n",
      "   [0.72156864 0.8745098  0.7607843 ]]]\n",
      "\n",
      "\n",
      " [[[0.49019608 0.3882353  0.4862745 ]\n",
      "   [0.4627451  0.36078432 0.45882353]\n",
      "   [0.41960785 0.32156864 0.40784314]\n",
      "   ...\n",
      "   [0.9372549  0.8980392  0.9019608 ]\n",
      "   [0.93333334 0.89411765 0.8980392 ]\n",
      "   [0.9490196  0.9098039  0.9137255 ]]\n",
      "\n",
      "  [[0.45490196 0.3529412  0.4509804 ]\n",
      "   [0.44705883 0.34901962 0.43529412]\n",
      "   [0.43137255 0.33333334 0.41960785]\n",
      "   ...\n",
      "   [0.94509804 0.90588236 0.9098039 ]\n",
      "   [0.9411765  0.9019608  0.90588236]\n",
      "   [0.95686275 0.91764706 0.92156863]]\n",
      "\n",
      "  [[0.44313726 0.34509805 0.43137255]\n",
      "   [0.4392157  0.34117648 0.42352942]\n",
      "   [0.43137255 0.33333334 0.41568628]\n",
      "   ...\n",
      "   [0.96862745 0.93333334 0.9372549 ]\n",
      "   [0.972549   0.9372549  0.9411765 ]\n",
      "   [0.98039216 0.94509804 0.9490196 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.07450981 0.07450981 0.07450981]\n",
      "   [0.07450981 0.07450981 0.07450981]\n",
      "   [0.07450981 0.07450981 0.07450981]\n",
      "   ...\n",
      "   [0.0627451  0.0627451  0.0627451 ]\n",
      "   [0.0627451  0.0627451  0.0627451 ]\n",
      "   [0.0627451  0.0627451  0.0627451 ]]\n",
      "\n",
      "  [[0.06666667 0.06666667 0.06666667]\n",
      "   [0.06666667 0.06666667 0.06666667]\n",
      "   [0.06666667 0.06666667 0.06666667]\n",
      "   ...\n",
      "   [0.0627451  0.0627451  0.0627451 ]\n",
      "   [0.0627451  0.0627451  0.0627451 ]\n",
      "   [0.0627451  0.0627451  0.0627451 ]]\n",
      "\n",
      "  [[0.0627451  0.0627451  0.0627451 ]\n",
      "   [0.0627451  0.0627451  0.0627451 ]\n",
      "   [0.0627451  0.0627451  0.0627451 ]\n",
      "   ...\n",
      "   [0.0627451  0.0627451  0.0627451 ]\n",
      "   [0.0627451  0.0627451  0.0627451 ]\n",
      "   [0.0627451  0.0627451  0.0627451 ]]]\n",
      "\n",
      "\n",
      " [[[0.9882353  0.85490197 0.37254903]\n",
      "   [0.99215686 0.85882354 0.3764706 ]\n",
      "   [1.         0.8666667  0.38039216]\n",
      "   ...\n",
      "   [0.9882353  0.827451   0.38039216]\n",
      "   [0.9882353  0.83137256 0.3529412 ]\n",
      "   [0.9843137  0.8392157  0.32941177]]\n",
      "\n",
      "  [[0.9843137  0.8509804  0.36862746]\n",
      "   [0.99215686 0.85882354 0.37254903]\n",
      "   [0.99607843 0.8627451  0.3764706 ]\n",
      "   ...\n",
      "   [0.99607843 0.83137256 0.39215687]\n",
      "   [1.         0.84705883 0.38039216]\n",
      "   [1.         0.85490197 0.37254903]]\n",
      "\n",
      "  [[0.9882353  0.84705883 0.3647059 ]\n",
      "   [0.99215686 0.8509804  0.36862746]\n",
      "   [0.99607843 0.8627451  0.3764706 ]\n",
      "   ...\n",
      "   [0.99607843 0.83137256 0.4       ]\n",
      "   [1.         0.84313726 0.40784314]\n",
      "   [1.         0.8509804  0.41568628]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.98039216 0.98039216 0.98039216]\n",
      "   [0.98039216 0.98039216 0.98039216]\n",
      "   [0.9843137  0.9843137  0.9843137 ]\n",
      "   ...\n",
      "   [0.98039216 0.827451   0.36078432]\n",
      "   [0.96862745 0.827451   0.35686275]\n",
      "   [0.9411765  0.8156863  0.33333334]]\n",
      "\n",
      "  [[0.972549   0.972549   0.972549  ]\n",
      "   [0.972549   0.972549   0.972549  ]\n",
      "   [0.9764706  0.9764706  0.9764706 ]\n",
      "   ...\n",
      "   [0.9607843  0.8156863  0.35686275]\n",
      "   [0.9764706  0.8392157  0.36862746]\n",
      "   [0.99215686 0.8745098  0.39607844]]\n",
      "\n",
      "  [[0.9647059  0.9647059  0.9647059 ]\n",
      "   [0.96862745 0.96862745 0.96862745]\n",
      "   [0.96862745 0.96862745 0.96862745]\n",
      "   ...\n",
      "   [0.9607843  0.8156863  0.35686275]\n",
      "   [0.9764706  0.8509804  0.38431373]\n",
      "   [1.         0.8980392  0.41960785]]]]\n",
      "(570, 224, 224, 3)\n",
      "[[[[0.14509805 0.18039216 0.6392157 ]\n",
      "   [0.13725491 0.17254902 0.6313726 ]\n",
      "   [0.12941177 0.16470589 0.62352943]\n",
      "   ...\n",
      "   [0.07058824 0.10588235 0.6039216 ]\n",
      "   [0.05882353 0.09411765 0.5921569 ]\n",
      "   [0.05098039 0.08627451 0.58431375]]\n",
      "\n",
      "  [[0.07450981 0.10980392 0.5764706 ]\n",
      "   [0.06666667 0.10196079 0.56078434]\n",
      "   [0.05882353 0.09411765 0.56078434]\n",
      "   ...\n",
      "   [0.         0.01960784 0.5176471 ]\n",
      "   [0.         0.01176471 0.50980395]\n",
      "   [0.         0.00784314 0.5058824 ]]\n",
      "\n",
      "  [[0.09803922 0.13333334 0.60784316]\n",
      "   [0.09019608 0.1254902  0.5921569 ]\n",
      "   [0.08235294 0.11764706 0.5921569 ]\n",
      "   ...\n",
      "   [0.01176471 0.04705882 0.54509807]\n",
      "   [0.01176471 0.04705882 0.54509807]\n",
      "   [0.01176471 0.04705882 0.54509807]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.00784314 0.03921569 0.56078434]\n",
      "   [0.01176471 0.04313726 0.5647059 ]\n",
      "   [0.01568628 0.04705882 0.5686275 ]\n",
      "   ...\n",
      "   [0.01960784 0.03921569 0.5411765 ]\n",
      "   [0.01960784 0.03921569 0.5411765 ]\n",
      "   [0.01568628 0.03529412 0.5372549 ]]\n",
      "\n",
      "  [[0.01568628 0.04705882 0.5686275 ]\n",
      "   [0.01568628 0.04705882 0.5686275 ]\n",
      "   [0.01960784 0.05098039 0.57254905]\n",
      "   ...\n",
      "   [0.01960784 0.03921569 0.5411765 ]\n",
      "   [0.01960784 0.03921569 0.5411765 ]\n",
      "   [0.01568628 0.03529412 0.5372549 ]]\n",
      "\n",
      "  [[0.01960784 0.05098039 0.57254905]\n",
      "   [0.01960784 0.05098039 0.57254905]\n",
      "   [0.01960784 0.05098039 0.57254905]\n",
      "   ...\n",
      "   [0.01960784 0.03921569 0.5411765 ]\n",
      "   [0.01960784 0.03921569 0.5411765 ]\n",
      "   [0.01568628 0.03529412 0.5372549 ]]]\n",
      "\n",
      "\n",
      " [[[0.6313726  0.6313726  0.6313726 ]\n",
      "   [0.84705883 0.84705883 0.84705883]\n",
      "   [0.827451   0.827451   0.827451  ]\n",
      "   ...\n",
      "   [0.8235294  0.8235294  0.8235294 ]\n",
      "   [0.8392157  0.8392157  0.8392157 ]\n",
      "   [0.7372549  0.7372549  0.7372549 ]]\n",
      "\n",
      "  [[0.78431374 0.78431374 0.78431374]\n",
      "   [1.         1.         1.        ]\n",
      "   [0.9882353  0.9882353  0.9882353 ]\n",
      "   ...\n",
      "   [0.9882353  0.9882353  0.9882353 ]\n",
      "   [1.         1.         1.        ]\n",
      "   [0.90588236 0.90588236 0.90588236]]\n",
      "\n",
      "  [[0.77254903 0.77254903 0.77254903]\n",
      "   [0.99607843 0.99607843 0.99607843]\n",
      "   [0.9882353  0.9882353  0.9882353 ]\n",
      "   ...\n",
      "   [0.99607843 0.99607843 0.99607843]\n",
      "   [1.         1.         1.        ]\n",
      "   [0.9098039  0.9098039  0.9098039 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.01176471 0.05098039 0.15686275]\n",
      "   [0.00392157 0.04313726 0.14901961]\n",
      "   [0.         0.03529412 0.14117648]\n",
      "   ...\n",
      "   [0.00784314 0.03921569 0.12941177]\n",
      "   [0.01176471 0.03529412 0.12941177]\n",
      "   [0.01176471 0.03529412 0.12156863]]\n",
      "\n",
      "  [[0.01176471 0.05098039 0.15686275]\n",
      "   [0.00392157 0.04313726 0.14901961]\n",
      "   [0.         0.03529412 0.14117648]\n",
      "   ...\n",
      "   [0.00784314 0.03921569 0.12941177]\n",
      "   [0.01176471 0.03529412 0.12156863]\n",
      "   [0.01568628 0.03529412 0.11372549]]\n",
      "\n",
      "  [[0.01176471 0.05098039 0.15686275]\n",
      "   [0.00392157 0.04313726 0.14901961]\n",
      "   [0.         0.03529412 0.14117648]\n",
      "   ...\n",
      "   [0.00784314 0.03921569 0.12156863]\n",
      "   [0.01176471 0.03529412 0.12156863]\n",
      "   [0.01568628 0.03529412 0.11372549]]]\n",
      "\n",
      "\n",
      " [[[1.         0.96862745 0.5058824 ]\n",
      "   [1.         0.95686275 0.49411765]\n",
      "   [1.         0.9411765  0.47843137]\n",
      "   ...\n",
      "   [0.99215686 0.98039216 0.43529412]\n",
      "   [0.99607843 0.9843137  0.3764706 ]\n",
      "   [1.         0.99215686 0.3529412 ]]\n",
      "\n",
      "  [[1.         0.9490196  0.4745098 ]\n",
      "   [1.         0.9411765  0.46666667]\n",
      "   [0.99607843 0.93333334 0.45882353]\n",
      "   ...\n",
      "   [1.         0.9882353  0.45490196]\n",
      "   [0.99607843 0.99215686 0.40392157]\n",
      "   [1.         1.         0.3882353 ]]\n",
      "\n",
      "  [[1.         0.93333334 0.45882353]\n",
      "   [1.         0.93333334 0.45882353]\n",
      "   [1.         0.92941177 0.45490196]\n",
      "   ...\n",
      "   [0.99607843 0.99607843 0.47843137]\n",
      "   [0.99215686 0.9843137  0.44705883]\n",
      "   [0.99215686 0.9882353  0.4392157 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.90588236 0.8352941  0.40392157]\n",
      "   [0.9098039  0.8392157  0.4       ]\n",
      "   [0.9137255  0.84705883 0.39215687]\n",
      "   ...\n",
      "   [1.         0.972549   0.54509807]\n",
      "   [1.         0.9764706  0.5411765 ]\n",
      "   [1.         0.98039216 0.5372549 ]]\n",
      "\n",
      "  [[0.99215686 0.9137255  0.4627451 ]\n",
      "   [0.9882353  0.92156863 0.45882353]\n",
      "   [0.99215686 0.92941177 0.4509804 ]\n",
      "   ...\n",
      "   [1.         0.99607843 0.54901963]\n",
      "   [1.         1.         0.54901963]\n",
      "   [1.         1.         0.5529412 ]]\n",
      "\n",
      "  [[0.99215686 0.91764706 0.45490196]\n",
      "   [1.         0.9254902  0.45490196]\n",
      "   [1.         0.9411765  0.4509804 ]\n",
      "   ...\n",
      "   [0.9372549  0.9098039  0.45490196]\n",
      "   [0.9490196  0.9098039  0.45882353]\n",
      "   [0.9529412  0.91764706 0.45882353]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.8        0.8627451  0.92156863]\n",
      "   [0.8        0.8627451  0.92156863]\n",
      "   [0.8        0.8627451  0.92156863]\n",
      "   ...\n",
      "   [0.87058824 0.92156863 0.95686275]\n",
      "   [0.87058824 0.92156863 0.95686275]\n",
      "   [0.87058824 0.92156863 0.95686275]]\n",
      "\n",
      "  [[0.8        0.8627451  0.92156863]\n",
      "   [0.8        0.8627451  0.92156863]\n",
      "   [0.8        0.8627451  0.92156863]\n",
      "   ...\n",
      "   [0.87058824 0.92156863 0.95686275]\n",
      "   [0.87058824 0.92156863 0.95686275]\n",
      "   [0.87058824 0.92156863 0.95686275]]\n",
      "\n",
      "  [[0.8        0.8627451  0.92156863]\n",
      "   [0.8        0.8627451  0.92156863]\n",
      "   [0.8        0.8627451  0.92156863]\n",
      "   ...\n",
      "   [0.87058824 0.92156863 0.95686275]\n",
      "   [0.87058824 0.92156863 0.95686275]\n",
      "   [0.87058824 0.92156863 0.95686275]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.2627451  0.5176471  0.68235296]\n",
      "   [0.2627451  0.5176471  0.68235296]\n",
      "   [0.2627451  0.5176471  0.68235296]\n",
      "   ...\n",
      "   [0.3647059  0.5803922  0.73333335]\n",
      "   [0.36862746 0.5764706  0.7411765 ]\n",
      "   [0.3647059  0.5803922  0.73333335]]\n",
      "\n",
      "  [[0.23137255 0.5137255  0.67058825]\n",
      "   [0.23137255 0.5137255  0.67058825]\n",
      "   [0.23137255 0.5137255  0.67058825]\n",
      "   ...\n",
      "   [0.3647059  0.57254905 0.7372549 ]\n",
      "   [0.36862746 0.5686275  0.74509805]\n",
      "   [0.3647059  0.57254905 0.7372549 ]]\n",
      "\n",
      "  [[0.23137255 0.52156866 0.6745098 ]\n",
      "   [0.23137255 0.52156866 0.6745098 ]\n",
      "   [0.23137255 0.52156866 0.6745098 ]\n",
      "   ...\n",
      "   [0.36078432 0.56078434 0.7372549 ]\n",
      "   [0.36078432 0.56078434 0.7372549 ]\n",
      "   [0.36078432 0.56078434 0.7372549 ]]]\n",
      "\n",
      "\n",
      " [[[0.29803923 0.33333334 0.3529412 ]\n",
      "   [0.36862746 0.40784314 0.41568628]\n",
      "   [0.36862746 0.4117647  0.39607844]\n",
      "   ...\n",
      "   [0.3137255  0.39215687 0.38431373]\n",
      "   [0.33333334 0.40784314 0.42352942]\n",
      "   [0.3764706  0.45490196 0.48235294]]\n",
      "\n",
      "  [[0.3764706  0.4        0.38431373]\n",
      "   [0.38039216 0.41960785 0.3882353 ]\n",
      "   [0.34509805 0.3764706  0.3254902 ]\n",
      "   ...\n",
      "   [0.31764707 0.38431373 0.34509805]\n",
      "   [0.28235295 0.35686275 0.34117648]\n",
      "   [0.2784314  0.34901962 0.34901962]]\n",
      "\n",
      "  [[0.38039216 0.39607844 0.30588236]\n",
      "   [0.34509805 0.36862746 0.26666668]\n",
      "   [0.2901961  0.30980393 0.19215687]\n",
      "   ...\n",
      "   [0.30588236 0.3529412  0.25882354]\n",
      "   [0.2901961  0.34117648 0.27058825]\n",
      "   [0.27450982 0.32941177 0.27058825]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.27058825 0.32941177 0.24705882]\n",
      "   [0.30980393 0.37254903 0.27058825]\n",
      "   [0.23921569 0.3019608  0.1882353 ]\n",
      "   ...\n",
      "   [0.2784314  0.33333334 0.24313726]\n",
      "   [0.26666668 0.32156864 0.25490198]\n",
      "   [0.26666668 0.32156864 0.2627451 ]]\n",
      "\n",
      "  [[0.22745098 0.29411766 0.21960784]\n",
      "   [0.30588236 0.3764706  0.28235295]\n",
      "   [0.24313726 0.31764707 0.20784314]\n",
      "   ...\n",
      "   [0.2784314  0.33333334 0.24313726]\n",
      "   [0.24313726 0.29803923 0.23137255]\n",
      "   [0.23137255 0.28627452 0.22745098]]\n",
      "\n",
      "  [[0.23137255 0.29803923 0.22352941]\n",
      "   [0.26666668 0.3372549  0.2509804 ]\n",
      "   [0.21960784 0.29411766 0.18431373]\n",
      "   ...\n",
      "   [0.21176471 0.26666668 0.1764706 ]\n",
      "   [0.23137255 0.28627452 0.21960784]\n",
      "   [0.27058825 0.3254902  0.26666668]]]\n",
      "\n",
      "\n",
      " [[[0.96862745 0.95686275 0.9372549 ]\n",
      "   [0.9647059  0.9529412  0.93333334]\n",
      "   [0.95686275 0.94509804 0.9254902 ]\n",
      "   ...\n",
      "   [0.9529412  0.9411765  0.92156863]\n",
      "   [0.95686275 0.94509804 0.9254902 ]\n",
      "   [0.9607843  0.9490196  0.92941177]]\n",
      "\n",
      "  [[0.9647059  0.9529412  0.93333334]\n",
      "   [0.9607843  0.9490196  0.92941177]\n",
      "   [0.95686275 0.94509804 0.9254902 ]\n",
      "   ...\n",
      "   [0.9607843  0.9490196  0.92941177]\n",
      "   [0.9647059  0.9529412  0.93333334]\n",
      "   [0.9647059  0.9529412  0.93333334]]\n",
      "\n",
      "  [[0.95686275 0.94509804 0.9254902 ]\n",
      "   [0.95686275 0.94509804 0.9254902 ]\n",
      "   [0.9529412  0.9411765  0.92156863]\n",
      "   ...\n",
      "   [0.9647059  0.9529412  0.93333334]\n",
      "   [0.96862745 0.95686275 0.9372549 ]\n",
      "   [0.96862745 0.95686275 0.9372549 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.8156863  0.78431374 0.7411765 ]\n",
      "   [0.81960785 0.7882353  0.74509805]\n",
      "   [0.81960785 0.7882353  0.74509805]\n",
      "   ...\n",
      "   [0.9490196  0.94509804 0.92941177]\n",
      "   [0.94509804 0.9411765  0.9254902 ]\n",
      "   [0.94509804 0.9411765  0.9254902 ]]\n",
      "\n",
      "  [[0.8        0.76862746 0.7254902 ]\n",
      "   [0.8039216  0.77254903 0.7294118 ]\n",
      "   [0.8156863  0.78431374 0.7411765 ]\n",
      "   ...\n",
      "   [0.9529412  0.9490196  0.93333334]\n",
      "   [0.9490196  0.94509804 0.92941177]\n",
      "   [0.94509804 0.9411765  0.9254902 ]]\n",
      "\n",
      "  [[0.78431374 0.7529412  0.70980394]\n",
      "   [0.7921569  0.7607843  0.7176471 ]\n",
      "   [0.80784315 0.7764706  0.73333335]\n",
      "   ...\n",
      "   [0.95686275 0.9529412  0.9372549 ]\n",
      "   [0.9529412  0.9490196  0.93333334]\n",
      "   [0.9490196  0.94509804 0.92941177]]]]\n"
     ]
    }
   ],
   "source": [
    "# Define the batch size\n",
    "batch_size = 100\n",
    "\n",
    "# Create a list of arrays to process\n",
    "arrays = [X_train_small, X_test_small]\n",
    "lst = []\n",
    "# Iterate over the arrays\n",
    "for i, arr in enumerate(arrays):\n",
    "    # Get the total number of images\n",
    "    num_images = len(arr)\n",
    "\n",
    "    # Calculate the total number of batches\n",
    "    num_batches = int(np.ceil(num_images / batch_size))\n",
    "\n",
    "    # Create an empty array to store the normalized images\n",
    "    normalized_arr = np.empty_like(arr, dtype=np.float32)\n",
    "\n",
    "    # Process images in batches\n",
    "    for j in range(num_batches):\n",
    "        start_idx = j * batch_size\n",
    "        end_idx = min((j + 1) * batch_size, num_images)\n",
    "\n",
    "        # Normalize the batch of images\n",
    "        batch_images = arr[start_idx:end_idx] / 255.\n",
    "\n",
    "        # Store the normalized batch in the result array\n",
    "        normalized_arr[start_idx:end_idx] = batch_images\n",
    "\n",
    "    # Assign the normalized array back to the original variable\n",
    "    arr = normalized_arr\n",
    "    lst.append(arr)\n",
    "\n",
    "    # Print the shape of the array after normalization\n",
    "    print(arr.shape)\n",
    "    print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_small = lst[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_small = lst[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot-Encode target y - **add this to preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id\n",
       "0         1\n",
       "1         4\n",
       "2        10\n",
       "3         9\n",
       "4        26\n",
       "         ..\n",
       "51295    17\n",
       "51296    17\n",
       "51297    17\n",
       "51298    17\n",
       "51299    17\n",
       "Name: Genre_id, Length: 51300, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_cat = to_categorical(y_train, num_classes=30)\n",
    "y_test_cat = to_categorical(y_test, num_classes=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_small_cat = to_categorical(y_train_small, num_classes=30)\n",
    "y_test_small_cat = to_categorical(y_test_small, num_classes=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((51300, 30), (5700, 30), (5130, 30), (570, 30))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_cat.shape, y_test_cat.shape, y_train_small_cat.shape, y_test_small_cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((51300, 224, 224, 3), (51300, 30))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train_cat.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((51300, 224, 224, 3), (51300, 30)), ((5700, 224, 224, 3), (5700, 30)))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train.shape, y_train_cat.shape), (X_test.shape, y_test_cat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((5130, 224, 224, 3), (5130, 30)), ((570, 224, 224, 3), (570, 30)))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train_small.shape, y_train_small_cat.shape), (X_test_small.shape, y_test_small_cat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_path = \"../raw_data/train_x_prep.csv\"\n",
    "#num_samples, width, height, channels = X_train.shape\n",
    "#X_train_2d = np.reshape(X_train, (num_samples, width * height * channels))\n",
    "#df_X_train = pd.DataFrame(X_train_2d)\n",
    "# PENDING TO SAVE ALL DATASETS AS CSVs\n",
    "#df_X_train.to_csv(X_train_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../raw_data/tmp_data/test_x_prep.npy'\n",
    "np.save(file_path, X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN - dummy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model():\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    # Convolution 1 & MaxPooling\n",
    "    model.add(layers.Conv2D(8, (4,4), input_shape=(224, 224, 3), padding='same', activation=\"relu\"))\n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "    \n",
    "    # Convolution 2 & MaxPooling\n",
    "    model.add(layers.Conv2D(32, (3,3), padding='same', activation=\"relu\"))\n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "    \n",
    "    # Convolution 3 & MaxPooling\n",
    "    model.add(layers.Conv2D(64, (3,3), padding='same', activation=\"relu\"))\n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "    \n",
    "    # Convolution 4 & MaxPooling\n",
    "    model.add(layers.Conv2D(64, (2,2), padding='same', activation=\"relu\"))\n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "    \n",
    "    # Flattening\n",
    "    model.add(layers.Flatten())\n",
    "    \n",
    "    # Hidden Dense layer\n",
    "    model.add(layers.Dense(60, activation='relu'))\n",
    "    model.add(layers.Dropout(rate=0.2))\n",
    "    \n",
    "    ### Last layer - Classification Layer with 30 outputs corresponding to 30 digits\n",
    "    model.add(layers.Dense(30, activation='softmax'))\n",
    "    \n",
    "    ## Model compilation\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                 optimizer='adam',\n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 200, 200, 8)       392       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 100, 100, 8)       0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 100, 100, 32)      2336      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 50, 50, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 50, 50, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 25, 25, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 25, 25, 64)        16448     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 12, 12, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 9216)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 60)                553020    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 60)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 30)                1830      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 592522 (2.26 MB)\n",
      "Trainable params: 592522 (2.26 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = initialize_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "129/129 [==============================] - 31s 233ms/step - loss: 3.4021 - accuracy: 0.0307 - val_loss: 3.4011 - val_accuracy: 0.0205\n",
      "Epoch 2/50\n",
      "129/129 [==============================] - 31s 238ms/step - loss: 3.3995 - accuracy: 0.0346 - val_loss: 3.3972 - val_accuracy: 0.0361\n",
      "Epoch 3/50\n",
      "129/129 [==============================] - 28s 220ms/step - loss: 3.3901 - accuracy: 0.0400 - val_loss: 3.4036 - val_accuracy: 0.0419\n",
      "Epoch 4/50\n",
      "129/129 [==============================] - 26s 205ms/step - loss: 3.3439 - accuracy: 0.0631 - val_loss: 3.3936 - val_accuracy: 0.0517\n",
      "Epoch 5/50\n",
      "129/129 [==============================] - 26s 201ms/step - loss: 3.2254 - accuracy: 0.1021 - val_loss: 3.4352 - val_accuracy: 0.0585\n",
      "Epoch 6/50\n",
      "129/129 [==============================] - 27s 206ms/step - loss: 2.9870 - accuracy: 0.1659 - val_loss: 3.5638 - val_accuracy: 0.0682\n",
      "Epoch 7/50\n",
      "129/129 [==============================] - 28s 214ms/step - loss: 2.6145 - accuracy: 0.2702 - val_loss: 3.8180 - val_accuracy: 0.0643\n",
      "Epoch 8/50\n",
      "129/129 [==============================] - 27s 210ms/step - loss: 2.2031 - accuracy: 0.3816 - val_loss: 4.5294 - val_accuracy: 0.0731\n",
      "Epoch 9/50\n",
      "129/129 [==============================] - 27s 211ms/step - loss: 1.8228 - accuracy: 0.4790 - val_loss: 5.2589 - val_accuracy: 0.0819\n",
      "Epoch 10/50\n",
      "129/129 [==============================] - 28s 217ms/step - loss: 1.5183 - accuracy: 0.5631 - val_loss: 5.8334 - val_accuracy: 0.0692\n",
      "Epoch 11/50\n",
      "129/129 [==============================] - 29s 226ms/step - loss: 1.3098 - accuracy: 0.6238 - val_loss: 7.1389 - val_accuracy: 0.0760\n",
      "Epoch 12/50\n",
      "129/129 [==============================] - 28s 214ms/step - loss: 1.1556 - accuracy: 0.6759 - val_loss: 7.3128 - val_accuracy: 0.0780\n",
      "Epoch 13/50\n",
      "129/129 [==============================] - 27s 208ms/step - loss: 1.0060 - accuracy: 0.7130 - val_loss: 7.5860 - val_accuracy: 0.0721\n",
      "Epoch 14/50\n",
      "129/129 [==============================] - 27s 206ms/step - loss: 0.9283 - accuracy: 0.7364 - val_loss: 8.1974 - val_accuracy: 0.0760\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x28859b3a0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = initialize_model()\n",
    "es = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "model.fit(X_train_small, y_train_small_cat,\n",
    "         validation_split = 0.2,\n",
    "         epochs=50, \n",
    "         batch_size=32,\n",
    "         verbose=1, \n",
    "         callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 1s 48ms/step - loss: 3.3674 - accuracy: 0.0386\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.038596492260694504"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test_small, y_test_small_cat)[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.8% accuracy in first model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((51300, 200, 200, 3), (5130, 200, 200, 3), (5130,))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_train_small.shape, y_train_small.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(16, (3,3), input_shape=(224, 224, 3), padding='same', strides=(1,1), activation='relu'))\n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(layers.Conv2D(32, (2,2), padding='same', strides=(1,1), activation='relu'))\n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(layers.Conv2D(64, (2,2), padding='same', strides=(1,1), activation='relu'))\n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    model.add(layers.Dense(50, activation='relu'))\n",
    "    model.add(layers.Dropout(rate=0.2))\n",
    "    \n",
    "    model.add(layers.Dense(30, activation='softmax'))\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " 67/129 [==============>...............] - ETA: 10s - loss: 3.4314 - accuracy: 0.0350"
     ]
    }
   ],
   "source": [
    "model = initialize_model()\n",
    "\n",
    "es = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(X_train_small, y_train_small_cat,\n",
    "         validation_split = 0.2,\n",
    "         epochs=50, \n",
    "         batch_size=32,\n",
    "         verbose=1, \n",
    "         callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, title='', axs=None, exp_name=\"\"):\n",
    "    if axs is not None:\n",
    "        ax1, ax2 = axs\n",
    "    else:\n",
    "        f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    if len(exp_name) > 0 and exp_name[0] != '_':\n",
    "        exp_name = '_' + exp_name\n",
    "    ax1.plot(history.history['loss'], label = 'train' + exp_name)\n",
    "    ax1.plot(history.history['val_loss'], label = 'val' + exp_name)\n",
    "    ax1.set_ylim(0., 2.2)\n",
    "    ax1.set_title('loss')\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.plot(history.history['accuracy'], label='train accuracy'  + exp_name)\n",
    "    ax2.plot(history.history['val_accuracy'], label='val accuracy'  + exp_name)\n",
    "    ax2.set_ylim(0.25, 1.)\n",
    "    ax2.set_title('Accuracy')\n",
    "    ax2.legend()\n",
    "    return (ax1, ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<AxesSubplot:title={'center':'loss'}>,\n",
       " <AxesSubplot:title={'center':'Accuracy'}>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179/179 [==============================] - 2s 13ms/step - loss: 3.1592 - accuracy: 0.1249\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.12491228431463242"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test_small, y_test_small_cat)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lewagon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
